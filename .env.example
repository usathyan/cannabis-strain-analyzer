# Use Ollama for local LLM inference
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=gemma3:latest

# Optional: OpenRouter for cloud-based inference
# OPENROUTER_API_KEY=your_openrouter_api_key_here

# Default location settings
DEFAULT_LOCATION="San Francisco, CA"
DEFAULT_RADIUS_MILES=25

# Optional: External API keys for enhanced data
# CANNLYTICS_API_KEY=your_cannlytics_api_key_here