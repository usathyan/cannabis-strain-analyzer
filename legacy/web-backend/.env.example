# Use Ollama for local LLM inference
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=gemma3:latest

# Optional: OpenRouter for cloud-based inference
# OPENROUTER_API_KEY=your_openrouter_api_key_here
