# Use Ollama for local LLM inference
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=gemma3:latest

# Default location settings
DEFAULT_LOCATION="San Francisco, CA"
DEFAULT_RADIUS_MILES=25